# Context-Aware Multimodal

## Context-Aware Multimodal Processing in RAGAnything



This document describes the context-aware multimodal processing feature in RAGAnything, which provides surrounding content information to LLMs when analyzing images, tables, equations, and other multimodal content for enhanced accuracy and relevance.

### Overview



The context-aware feature enables RAGAnything to automatically extract and provide surrounding text content as context when processing multimodal content. This leads to more accurate and contextually relevant analysis by giving AI models additional information about where the content appears in the document structure.

#### Key Benefits



* **Enhanced Accuracy**: Context helps AI understand the purpose and meaning of multimodal content
* **Semantic Coherence**: Generated descriptions align with document context and terminology
* **Automated Integration**: Context extraction is automatically enabled during document processing
* **Flexible Configuration**: Multiple extraction modes and filtering options

\






[![GitBook](https://img.shields.io/static/v1?message=Documented%20on%20GitBook\&logo=gitbook\&logoColor=ffffff\&label=%20\&labelColor=5c5c5c\&color=3F89A1)](https://www.gitbook.com/preview?utm_source=gitbook_readme_badge\&utm_medium=organic\&utm_campaign=preview_documentation\&utm_content=link)
